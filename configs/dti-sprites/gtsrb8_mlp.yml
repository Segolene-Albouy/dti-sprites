dataset:
  name: gtsrb8
model:
  name: dti_sprites
  n_sprites: 8
  n_backgrounds: 1
  prototype:
    source: generator
    generator: mlp
    data:
      freeze: [False, False, False]
      init: ["", "constant", ""]
  encoder_name: resnet32
  transformation_sequence: identity_color_projective_tps
  transformation_sequence_bkg: identity_color_projective_tps
  curriculum_learning: [100, 300, 800]
  curriculum_learning_bkg: [100, 300, 800]
training:
  batch_size: 128
  n_workers: 4
  optimizer:
    name: adam
    lr: 1.0e-3
    transformer:
      weight_decay: 1.0e-6
  scheduler:
    name: multi_step
    gamma: 0.1
    milestones: [1300]
  n_epoches: 1500
  train_stat_interval: 100
  val_stat_interval: 200
  check_cluster_interval: 25
  seed: 4820
  visualizer_port:
  pretrained: 
  resume: 
